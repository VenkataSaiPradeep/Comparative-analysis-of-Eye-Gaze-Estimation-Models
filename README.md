# Comparative Study and Analysis of Eye Gaze Estimation Models: iTracker, ODABE, and FAZE

## ğŸ“Œ Overview

This repository presents a comparative study of three prominent eye gaze estimation models: **iTracker**, **ODABE**, and **FAZE**. Eye gaze estimation plays a crucial role in various applications, including human-computer interaction, assistive technologies, and virtual reality. Our objective is to analyze the performance, accuracy, and generalization capability of these models under various conditions.

## âœ¨ Models in Focus

1. **iTracker**: A CNN-based approach that utilizes head pose and eye images for gaze prediction, designed for consumer-grade devices.
2. **ODABE**: A robust, deep-learning-based model that uses multi-view geometry for accurate gaze prediction across a wide range of head poses and lighting conditions.
3. **FAZE**: A cutting-edge few-shot learning-based model designed to generalize to new users with minimal calibration data.

## ğŸ› ï¸ Features

- **Data Preprocessing**: Includes scripts for preparing datasets from widely used benchmarks such as GazeCapture and MPIIGaze.
- **Model Implementation**: Reproduces the original architectures of iTracker, ODABE, and FAZE for side-by-side comparison.
- **Performance Metrics**: Evaluation using metrics such as Mean Angular Error (MAE) and inference speed under different hardware setups.
- **Visualization Tools**: Heatmaps and error distribution graphs for intuitive understanding of model behavior.

## ğŸ“Š Key Findings

- **Accuracy**: Comparison of average gaze estimation error across models.
- **Generalization**: Performance of each model on unseen users and environments.
- **Computation**: Evaluation of inference time and resource consumption.
  
## ğŸ‘¥ Team Members
- **Venkata Sai Pradeep Nagisetti**
- **Dr. Abdallah Moubayed**
- **Pavithra Moravaneni**
- **Loka Kalyan Balla**
- **Sowmya Veldandi]** 
- **Kethan Yeddla** 
- **Sai Teja Madha**

## ğŸ“š References
- 1.] iTracker: Eye Tracking for Everyone K. Krafka et al., â€œEye Tracking for Everyone,â€ www.cv-foundation.org,2016.
- 2.] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, â€œMPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 1, pp.162â€“175, Jan. 2019, doi: https://doi.org/10.1109/tpami.2017.2778103.
- 3.] S. Park, S. D. Mello, P. Molchanov, U. Iqbal, O. Hilliges, and J. Kautz,â€œFew-Shot Adaptive Gaze Estimation,â€ openaccess.thecvf.com, 2019.
- 4.] Bruno Klein Salvalaio and Gabriel, â€œSelf-Adaptive AppearanceBased Eye-Tracking with Online Transfer Learning,â€ Oct. 2019, doi:https://doi.org/10.1109/bracis.2019.00074.
- 5.] Seonwook Park and Shalini De Mello and Pavlo Molchanov and Umar Iqbal and Otmar Hilliges and Jan Kautz â€Few-Shot Adaptive Gaze Estimation,â€ â€International Conference on Computer Vision (ICCV)â€
- 6.] S. A. Mostafa and I. A. Ahmad, â€œRecent developments in systematic sampling: A review,â€ Journal of Statistical Theory and Practice, vol. 12, no. 2, pp. 290â€“310, Aug. 2017, doi:https://doi.org/10.1080/15598608.2017.1353456.
- 7.] D. E. King, â€œDlib-ml: A machine learning toolkit,â€ Journal of Machine Learning Research, vol. 10, pp. 1755â€“1758, 2009.
- 8.] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, â€œImageNet: A Large-Scale Hierarchical Image Database,â€ in CVPR09, 2009.
